{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import pickle as pkl\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\"The base class for all datasets.\n",
    "    \n",
    "    Every dataset class should inherit from Dataset \n",
    "    and load the data. Dataset only declaires the attributes.\n",
    "    \n",
    "    Attributes:\n",
    "        train_data: A numpy array with data that can be labelled.\n",
    "        train_labels: A numpy array with labels of train_data.\n",
    "        test_data: A numpy array with data that will be used for testing.\n",
    "        test_labels: A numpy array with labels of test_data.\n",
    "        n_state_estimation: An integer indicating #datapoints reserved for state representation estimation.\n",
    "        distances: A numpy array with pairwise Eucledian distances between all train_data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_state_estimation):\n",
    "        \"\"\"Inits the Dataset object and initialises the attributes with given or empty values.\"\"\"\n",
    "        self.train_data = np.array([[]])\n",
    "        self.train_labels = np.array([[]])\n",
    "        self.test_data = np.array([[]])\n",
    "        self.test_labels = np.array([[]])\n",
    "        self.n_state_estimation = n_state_estimation\n",
    "        self.regenerate()\n",
    "        \n",
    "    def regenerate(self):\n",
    "        \"\"\"The function for generating a dataset with new parameters.\"\"\"\n",
    "        pass\n",
    "        \n",
    "    def _scale_data(self):\n",
    "        \"\"\"Scales train data to 0 mean and unit variance. Test data is scaled with parameters of train data.\"\"\"\n",
    "        self.train_data = self.train_data/255.0\n",
    "        self.test_data = self.test_data/255.0\n",
    "        #scaler = preprocessing.StandardScaler().fit(self.train_data)\n",
    "        #self.train_data = scaler.transform(self.train_data)\n",
    "        #self.test_data = scaler.transform(self.test_data)\n",
    "        \n",
    "    def _keep_state_data(self):\n",
    "        \"\"\"self.n_state_estimation samples in training data are reserved for estimating the state.\"\"\"\n",
    "        self.train_data, self.state_data, self.train_labels, self.state_labels = train_test_split(\n",
    "            self.train_data, self.train_labels, test_size=self.n_state_estimation)\n",
    "        \n",
    "    #def _compute_distances(self):\n",
    "    #    print(\"computing distance...\")\n",
    "    #    \"\"\"Computes the pairwise distances between all training datapoints\"\"\"\n",
    "    #    self.distances = scipy.spatial.distance.pdist(self.train_data, metric='cosine')\n",
    "    #    self.distances = scipy.spatial.distance.squareform(self.distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_train(Dataset):      \n",
    "    \"\"\"\n",
    "    Added by Seungbo\n",
    "    \n",
    "    Attributes:\n",
    "        possible_names: A list indicating the dataset names that can be used.\n",
    "        subset: An integer indicating what subset of data to use. 0: even, 1: odd, -1: all datapoints. \n",
    "        size: An integer indicating the size of training dataset to sample, if -1 use all data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_state_estimation, subset, size=-1):\n",
    "        \"\"\"Inits a few attributes and the attributes of Dataset object.\"\"\"\n",
    "        self.subset = subset\n",
    "        self.size = size\n",
    "        Dataset.__init__(self, n_state_estimation) \n",
    "    \n",
    "    def regenerate(self):\n",
    "        \"\"\"Loads the data and split it into train and test.\"\"\"\n",
    "        # load data\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (X, y), _ = mnist.load_data()\n",
    "        X = X.reshape((len(X), 28, 28, 1))\n",
    "        #X = X.reshape(len(X), -1)\n",
    "        dtst_size = len(y)\n",
    "        \n",
    "        # even datapoints subset\n",
    "        if self.subset == 0:\n",
    "            valid_indeces = list(range(0, dtst_size, 2))\n",
    "        # odd datapoints subset\n",
    "        elif self.subset == 1:\n",
    "            valid_indeces = list(range(1, dtst_size, 2))\n",
    "        # all datapoints\n",
    "        elif self.subset == -1:\n",
    "            valid_indeces = list(range(dtst_size))\n",
    "        else:\n",
    "            print('Incorrect subset attribute value!')\n",
    "        \n",
    "        # try to split data into training and test subsets while insuring that \n",
    "        # all classes from test data are present in train data \n",
    "        \n",
    "        # get a part of dataset according to subset (even, odd or all)\n",
    "        train_test_data = X[valid_indeces,:]\n",
    "        train_test_labels = y[valid_indeces]\n",
    "        # use a random half/half split for train and test data\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=0)\n",
    "        \n",
    "        for train_index, test_index in sss.split(train_test_data, train_test_labels):\n",
    "            self.train_data,self.test_data = X[train_index], X[test_index]\n",
    "            self.train_labels, self.test_labels = y[train_index], y[test_index]\n",
    "\n",
    "        self._scale_data()\n",
    "        self._keep_state_data()\n",
    "        #self._compute_distances()\n",
    "\n",
    "        # keep only a part of data for training\n",
    "        self.train_data = self.train_data.astype(np.float32)\n",
    "        self.train_data = self.train_data[:self.size,:]\n",
    "        self.train_labels = self.train_labels[:self.size]\n",
    "        self.train_labels = to_categorical(self.train_labels, 10)\n",
    "        # this is needed to insure that some of the classes are missing in train or test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_test(Dataset):      \n",
    "    \"\"\"\n",
    "    Added by Seungbo\n",
    "    \n",
    "    Attributes:\n",
    "        possible_names: A list indicating the dataset names that can be used.\n",
    "        subset: An integer indicating what subset of data to use. 0: even, 1: odd, -1: all datapoints. \n",
    "        size: An integer indicating the size of training dataset to sample, if -1 use all data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_state_estimation, subset, size=-1):\n",
    "        \"\"\"Inits a few attributes and the attributes of Dataset object.\"\"\"\n",
    "        self.subset = subset\n",
    "        self.size = size\n",
    "        Dataset.__init__(self, n_state_estimation) \n",
    "    \n",
    "    def regenerate(self):\n",
    "        \"\"\"Loads the data and split it into train and test.\"\"\"\n",
    "        # load data\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        _, (X, y) = mnist.load_data()\n",
    "        X = X.reshape((len(X), 28, 28, 1))\n",
    "        #X = X.reshape(len(X), -1)\n",
    "        dtst_size = len(y)\n",
    "        \n",
    "        # even datapoints subset\n",
    "        if self.subset == 0:\n",
    "            valid_indeces = list(range(0, dtst_size, 2))\n",
    "        # odd datapoints subset\n",
    "        elif self.subset == 1:\n",
    "            valid_indeces = list(range(1, dtst_size, 2))\n",
    "        # all datapoints\n",
    "        elif self.subset == -1:\n",
    "            valid_indeces = list(range(dtst_size))\n",
    "        else:\n",
    "            print('Incorrect subset attribute value!')\n",
    "        \n",
    "        # try to split data into training and test subsets while insuring that \n",
    "        # all classes from test data are present in train data \n",
    "        \n",
    "        # get a part of dataset according to subset (even, odd or all)\n",
    "        train_test_data = X[valid_indeces,:]\n",
    "        train_test_labels = y[valid_indeces]\n",
    "        # use a random half/half split for train and test data\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=0)\n",
    "        \n",
    "        for train_index, test_index in sss.split(train_test_data, train_test_labels):\n",
    "            self.train_data,self.test_data = X[train_index], X[test_index]\n",
    "            self.train_labels, self.test_labels = y[train_index], y[test_index]\n",
    "\n",
    "        self._scale_data()\n",
    "        self._keep_state_data()\n",
    "        #self._compute_distances()\n",
    "\n",
    "        # keep only a part of data for training\n",
    "        self.train_data = self.train_data.astype(np.float32)\n",
    "        self.train_data = self.train_data[:self.size,:]\n",
    "        self.train_labels = self.train_labels[:self.size]\n",
    "        self.train_labels = to_categorical(self.train_labels, 10)        \n",
    "        # this is needed to insure that some of the classes are missing in train or test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (envs.py, line 209)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/seungbo/anaconda3/envs/DRL/lib/python3.7/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3437\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-27edb37c93e5>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from envs import LalEnvTargetAccuracy\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/seungbo/DRL/LAL-RL/envs.py\"\u001b[0;36m, line \u001b[0;32m209\u001b[0m\n\u001b[0;31m    a2 = tf.nn.softmax(output) *\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# depending on the classification model use, we might need to import other packages\n",
    "# from sklearn import svm\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#from datasets import DatasetUCI\n",
    "\n",
    "from envs import LalEnvTargetAccuracy\n",
    "\n",
    "from helpers import Minibatch, ReplayBuffer\n",
    "from dqn import DQN\n",
    "from Test_AL import policy_rl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Parameters for dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STATE_ESTIMATION = 30\n",
    "SIZE = 100\n",
    "# if we want to train and test RL on the same dataset, use even and odd datapoints for training and testing correspondingly\n",
    "SUBSET = -1 # -1 for using all datapoints, 0 for even, 1 for odd\n",
    "N_JOBS = 1 # can set more if we want to parallelise\n",
    "\n",
    "# The quality is measures according to a given quality measure `quality_method`. \n",
    "QUALITY_METHOD = metrics.accuracy_score\n",
    "# The `tolerance_level` is the proportion of max quality that needs to be achived in order to terminate an episode. \n",
    "TOLERANCE_LEVEL = 0.98"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise a dataset that will contain a sample of datapoint from one the indicated classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST_train(n_state_estimation=N_STATE_ESTIMATION, subset=SUBSET, size=SIZE)\n",
    "dataset_test = MNIST_test(n_state_estimation=N_STATE_ESTIMATION, subset=1, size=SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise a model that would be used for training a classifier. <br>\n",
    "It can be, for example, Logistic regression: <br>\n",
    "`LogisticRegression(n_jobs=N_JOBS)` <br>\n",
    "SVM: <br>\n",
    "`svm.SVC(probability=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTModel (tf.keras.Model): # keras.model 구현\n",
    "    def __init__(self):  # 기본이 되는 층을 구현\n",
    "        # call the parent constructor(class의 tf.keras.Model) \n",
    "        super(MNISTModel, self).__init__() \n",
    "        # initialize the layers\n",
    "        self.conv1 = keras.layers.Conv2D(filters=32, kernel_size=[3, 3], padding='SAME', activation=tf.nn.relu)\n",
    "        self.pool1 = keras.layers.MaxPool2D(padding='SAME')\n",
    "        self.conv2 = keras.layers.Conv2D(filters=64, kernel_size=[3, 3], padding='SAME', activation=tf.nn.relu)\n",
    "        self.pool2 = keras.layers.MaxPool2D(padding='SAME')\n",
    "        self.conv3 = keras.layers.Conv2D(filters=128, kernel_size=[3, 3], padding='SAME', activation=tf.nn.relu)\n",
    "        self.pool3 = keras.layers.MaxPool2D(padding='SAME')\n",
    "        self.pool3_flat = keras.layers.Flatten()\n",
    "        self.dense4 = keras.layers.Dense(units=256, activation=tf.nn.relu)\n",
    "        self.drop4 = keras.layers.Dropout(rate=0.4)\n",
    "        self.dense5 = keras.layers.Dense(units=10, activation=tf.nn.softmax)\n",
    "    # init에서 만든 층을 불러와서 network 구성 (연산부분을 담당)   \n",
    "    def call(self, inputs, training=False):  # training : training과 test시에 다르게 동작할 때, true면 둘이 동일하게 사용됨\n",
    "        net = self.conv1(inputs)\n",
    "        net = self.pool1(net)\n",
    "        net = self.conv2(net)\n",
    "        net = self.pool2(net)\n",
    "        net = self.conv3(net)\n",
    "        net = self.pool3(net)\n",
    "        net = self.pool3_flat(net)\n",
    "        net = self.dense4(net)\n",
    "        net = self.drop4(net)\n",
    "        net = self.dense5(net)\n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.train_labels[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(dataset.train_data, dataset.train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(100), Dimension(10)])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(dataset.train_data, dataset.train_labels,)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(dataset.train_data, dataset.train_labels,\n",
    "         batch_size=32,\n",
    "         epochs=5,\n",
    "         verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = LalEnvTargetAccuracy(dataset, model, quality_method=QUALITY_METHOD, tolerance_level=TOLERANCE_LEVEL)\n",
    "env_test = LalEnvTargetAccuracy(dataset_test, model, quality_method=QUALITY_METHOD, tolerance_level=TOLERANCE_LEVEL)\n",
    "#tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters for training RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRNAME = './agents/cnn_mnist_jupyter/' # The resulting agent of this experiment will be written in a file\n",
    "\n",
    "# Replay buffer parameters.\n",
    "REPLAY_BUFFER_SIZE = 1e4\n",
    "PRIOROTIZED_REPLAY_EXPONENT = 3\n",
    "\n",
    "# Agent parameters.\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3\n",
    "TARGET_COPY_FACTOR = 0.01\n",
    "BIAS_INITIALIZATION = 0 # default 0 # will be set to minus half of average duration during warm start experiemnts\n",
    "\n",
    "# Warm start parameters.\n",
    "WARM_START_EPISODES = 128 # reduce for test\n",
    "NN_UPDATES_PER_WARM_START = 100\n",
    "\n",
    "# Episode simulation parameters.\n",
    "EPSILON_START = 1\n",
    "EPSILON_END = 0.1\n",
    "EPSILON_STEPS = 1000\n",
    "\n",
    "# Training parameters\n",
    "TRAINING_ITERATIONS = 1000 # reduce for test\n",
    "TRAINING_EPISODES_PER_ITERATION = 10 # at each training ietration x episodes are simulated\n",
    "NN_UPDATES_PER_ITERATION = 60 # at each training iteration x gradient steps are made\n",
    "\n",
    "# Validation and test parameters\n",
    "N_VALIDATION = 500 # reduce for test\n",
    "N_TEST = 500 # reduce for test\n",
    "VALIDATION_TEST_FREQUENCY = 100 # every x iterations val and test are performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(buffer_size=REPLAY_BUFFER_SIZE, \n",
    "                             prior_exp=PRIOROTIZED_REPLAY_EXPONENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warm start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warm-start the replay buffer with random episodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(model):\n",
    "    \"\"\"Initialize weights of Neural Networks\n",
    "    \"\"\"\n",
    "    session = keras.backend.get_session()\n",
    "    for layer in model.layers: \n",
    "        if hasattr(layer, 'kernel_initializer'):\n",
    "            layer.kernel.initializer.run(session=session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of episode duration to compute average\n",
    "episode_durations = []\n",
    "for _ in range(WARM_START_EPISODES):\n",
    "    print('.', end='')\n",
    "    #reset_weights(model)\n",
    "    \n",
    "    # Reset the environment to start a new episode\n",
    "    # classifier_state contains vector representation of state of the environment (depends on classifier)\n",
    "    # next_action_state contains vector representations of all actions available to be taken at the next step\n",
    "    classifier_state, next_action_state = env.reset(n_start=10)\n",
    "    terminal = False\n",
    "    episode_duration = 0\n",
    "    # before we reach a terminal state, make steps\n",
    "    while not terminal:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(0, env.n_actions)\n",
    "        # taken_action_state is a vector corresponding to a taken action\n",
    "        taken_action_state = next_action_state[:,action]\n",
    "        next_classifier_state, next_action_state, reward, terminal = env.step(action)\n",
    "        # Store the transition in the replay buffer\n",
    "        replay_buffer.store_transition(classifier_state, \n",
    "                                       taken_action_state, \n",
    "                                       reward, next_classifier_state, \n",
    "                                       next_action_state, terminal)\n",
    "        # Get ready for next step\n",
    "        classifier_state = next_classifier_state\n",
    "        episode_duration += 1 \n",
    "    episode_durations.append(episode_duration)\n",
    "# compute the average episode duration of episodes generated during the warm start procedure\n",
    "av_episode_duration = np.mean(episode_durations)\n",
    "print('Average episode duration = ', av_episode_duration)\n",
    "\n",
    "BIAS_INITIALIZATION = -av_episode_duration/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agent = DQN(experiment_dir=DIRNAME,\n",
    "            observation_length=N_STATE_ESTIMATION,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            target_copy_factor=TARGET_COPY_FACTOR,\n",
    "            bias_average=BIAS_INITIALIZATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do updates of the network based on warm start episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _ in range(NN_UPDATES_PER_WARM_START):\n",
    "    print('.', end='')\n",
    "    # Sample a batch from the replay buffer proportionally to the probability of sampling.\n",
    "    minibatch = replay_buffer.sample_minibatch(BATCH_SIZE)\n",
    "    # Use batch to train an agent. Keep track of temporal difference errors during training.\n",
    "    td_error = agent.train(minibatch)\n",
    "    # Update probabilities of sampling each datapoint proportionally to the error.\n",
    "    replay_buffer.update_td_errors(td_error, minibatch.indeces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run multiple training iterations. Each iteration consits of:\n",
    "- generating episodes following agent's actions with exploration\n",
    "- validation and test episodes for evaluating performance\n",
    "- Q-network updates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episode_rewards = []\n",
    "i_episode = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for iteration in range(TRAINING_ITERATIONS):\n",
    "    # GENERATE NEW EPISODES\n",
    "    # Compute epsilon value according to the schedule.\n",
    "    epsilon = max(EPSILON_END, EPSILON_START-iteration*(EPSILON_START-EPSILON_END)/EPSILON_STEPS)\n",
    "    print(iteration, end=': ')\n",
    "    # Simulate training episodes.\n",
    "    for _ in range(TRAINING_EPISODES_PER_ITERATION):\n",
    "        # Reset the environment to start a new episode.\n",
    "        classifier_state, next_action_state = env.reset()\n",
    "        print(\".\", end='')\n",
    "        terminal = False\n",
    "        # Keep track of stats of episode to analyse it in tensorboard.\n",
    "        episode_reward = 0\n",
    "        episode_duration = 0\n",
    "        episode_summary = tf.Summary()\n",
    "        # Run an episode.\n",
    "        while not terminal:\n",
    "            # Let an agent choose an action.\n",
    "            action = agent.get_action(classifier_state, next_action_state)\n",
    "            # Get a prob of a datapoint corresponding to an action chosen by an agent.\n",
    "            # It is needed just for the tensorboard analysis.\n",
    "            rlchosen_action_state = next_action_state[0,action]\n",
    "            \n",
    "            # With epsilon probability, take a random action.\n",
    "            if np.random.ranf() < epsilon: \n",
    "                action = np.random.randint(0, env.n_actions)\n",
    "            # taken_action_state is a vector that corresponds to a taken action\n",
    "            taken_action_state = next_action_state[:,action]\n",
    "            # Make another step.\n",
    "            next_classifier_state, next_action_state, reward, terminal = env.step(action)\n",
    "            # Store a step in replay buffer\n",
    "            replay_buffer.store_transition(classifier_state, \n",
    "                                           taken_action_state, \n",
    "                                           reward, \n",
    "                                           next_classifier_state, \n",
    "                                           next_action_state, \n",
    "                                           terminal)\n",
    "            # Change a state of environment.\n",
    "            classifier_state = next_classifier_state\n",
    "            # Keep track of stats and add summaries to tensorboard.\n",
    "            episode_reward += reward\n",
    "            episode_duration += 1\n",
    "            episode_summary.value.add(simple_value=rlchosen_action_state, \n",
    "                                      tag=\"episode/rlchosen_action_state\")\n",
    "            episode_summary.value.add(simple_value=taken_action_state[0], \n",
    "                                      tag=\"episode/taken_action_state\")\n",
    "        # Add summaries to tensorboard\n",
    "        episode_summary.value.add(simple_value=episode_reward, \n",
    "                                  tag=\"episode/episode_reward\")\n",
    "        episode_summary.value.add(simple_value=episode_duration, \n",
    "                                  tag=\"episode/episode_duration\")\n",
    "        i_episode += 1\n",
    "        agent.summary_writer.add_summary(episode_summary, i_episode)\n",
    "        agent.summary_writer.flush()\n",
    "        \n",
    "    # VALIDATION AND TEST EPISODES\n",
    "    episode_summary = tf.Summary()\n",
    "    if iteration%VALIDATION_TEST_FREQUENCY == 0:\n",
    "        # Validation episodes are run. Use env for it.\n",
    "        all_durations = []\n",
    "        for i in range(N_VALIDATION):\n",
    "            done = False\n",
    "            state, next_action_state = env.reset()\n",
    "            while not(done):\n",
    "                action = policy_rl(agent, state, next_action_state)        \n",
    "                taken_action_state = next_action_state[:,action]\n",
    "                next_state, next_action_state, reward, done = env.step(action)\n",
    "                state = next_state\n",
    "            all_durations.append(len(env.episode_qualities))\n",
    "        episode_summary.value.add(simple_value=np.mean(all_durations), \n",
    "                                  tag=\"episode/train_duration\")\n",
    "        # Test episodes are run. Use env_test for it.\n",
    "        all_durations = []\n",
    "        for i in range(N_TEST):\n",
    "            done = False\n",
    "            state, next_action_state = env_test.reset()\n",
    "            while not(done):\n",
    "                action = policy_rl(agent, state, next_action_state)        \n",
    "                taken_action_state = next_action_state[:,action]\n",
    "                next_state, next_action_state, reward, done = env_test.step(action)\n",
    "                state = next_state\n",
    "            all_durations.append(len(env_test.episode_qualities))\n",
    "        episode_summary.value.add(simple_value=np.mean(all_durations), \n",
    "                                  tag=\"episode/test_duration\")\n",
    "    \n",
    "    episode_summary.value.add(simple_value=epsilon, \n",
    "                              tag=\"episode/epsilon\")\n",
    "    agent.summary_writer.add_summary(episode_summary, iteration)\n",
    "    agent.summary_writer.flush()\n",
    "            \n",
    "    # NEURAL NETWORK UPDATES\n",
    "    for _ in range(NN_UPDATES_PER_ITERATION):\n",
    "        minibatch = replay_buffer.sample_minibatch(BATCH_SIZE)\n",
    "        td_error = agent.train(minibatch)\n",
    "        replay_buffer.update_td_errors(td_error, minibatch.indeces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### To see the results in tensorboard\n",
    "\n",
    "on the server:\n",
    "tensorboard --logdir=./\n",
    "\n",
    "on the computer:\n",
    "ssh -N -f -L localhost:6006:localhost:6006 konyushk@iccvlabsrv20.iccluster.epfl.ch && open http://localhost:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
